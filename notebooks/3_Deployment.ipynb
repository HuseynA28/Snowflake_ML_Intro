{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.metrics import accuracy_score\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = session.table(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"SURVIVED\"  |\"PCLASS\"  |\"AGE\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\"ADULT_MALE\"  |\"DECK\"  |\"ALIVE\"  |\"ALONE\"  |\"SEX\"   |\"EMBARKED\"  |\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|0           |3         |22.00  |1        |0        |7.2500   |True          |NULL    |False    |False    |MALE    |S           |THIRD    |MAN    |SOUTHAMPTON    |\n",
      "|1           |1         |38.00  |1        |0        |71.2833  |False         |C       |True     |False    |FEMALE  |C           |FIRST    |WOMAN  |CHERBOURG      |\n",
      "|1           |3         |26.00  |0        |0        |7.9250   |False         |NULL    |True     |True     |FEMALE  |S           |THIRD    |WOMAN  |SOUTHAMPTON    |\n",
      "|1           |1         |35.00  |1        |0        |53.1000  |False         |C       |True     |False    |FEMALE  |S           |FIRST    |WOMAN  |SOUTHAMPTON    |\n",
      "|0           |3         |35.00  |0        |0        |8.0500   |True          |NULL    |False    |True     |MALE    |S           |THIRD    |MAN    |SOUTHAMPTON    |\n",
      "|0           |3         |NULL   |0        |0        |8.4583   |True          |NULL    |False    |True     |MALE    |Q           |THIRD    |MAN    |QUEENSTOWN     |\n",
      "|0           |1         |54.00  |0        |0        |51.8625  |True          |E       |False    |True     |MALE    |S           |FIRST    |MAN    |SOUTHAMPTON    |\n",
      "|0           |3         |2.00   |3        |1        |21.0750  |False         |NULL    |False    |False    |MALE    |S           |THIRD    |CHILD  |SOUTHAMPTON    |\n",
      "|1           |3         |27.00  |0        |2        |11.1333  |False         |NULL    |True     |False    |FEMALE  |S           |THIRD    |WOMAN  |SOUTHAMPTON    |\n",
      "|1           |2         |14.00  |1        |0        |30.0708  |False         |NULL    |True     |False    |FEMALE  |C           |SECOND   |CHILD  |CHERBOURG      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGE': 177, 'DECK': 688, 'EMBARKED': 2, 'EMBARK_TOWN': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns with null values and their respective counts\n",
    "{\n",
    "    k: v\n",
    "    for k, v in {\n",
    "        col_name: titanic_df.where(col(col_name).is_null()).count()\n",
    "        for col_name in titanic_df.columns\n",
    "    }.items()\n",
    "    if v > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\n",
    "    [\"AGE\", \"DECK\", \"ALIVE\", \"ADULT_MALE\", \"EMBARKED\", \"SEX\", \"PCLASS\", \"ALONE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\"FARE\"   |\n",
      "------------------------------------------------------------------------------\n",
      "|0           |1        |0        |THIRD    |MAN    |SOUTHAMPTON    |7.25     |\n",
      "|1           |1        |0        |FIRST    |WOMAN  |CHERBOURG      |71.2833  |\n",
      "|1           |0        |0        |THIRD    |WOMAN  |SOUTHAMPTON    |7.925    |\n",
      "|1           |1        |0        |FIRST    |WOMAN  |SOUTHAMPTON    |53.1     |\n",
      "|0           |0        |0        |THIRD    |MAN    |SOUTHAMPTON    |8.05     |\n",
      "|0           |0        |0        |THIRD    |MAN    |QUEENSTOWN     |8.4583   |\n",
      "|0           |0        |0        |FIRST    |MAN    |SOUTHAMPTON    |51.8625  |\n",
      "|0           |3        |1        |THIRD    |CHILD  |SOUTHAMPTON    |21.075   |\n",
      "|1           |0        |2        |THIRD    |WOMAN  |SOUTHAMPTON    |11.1333  |\n",
      "|1           |1        |0        |SECOND   |CHILD  |CHERBOURG      |30.0708  |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df = titanic_df.withColumn(\"FARE\", titanic_df[\"FARE\"].astype(T.FloatType()))\n",
    "\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"CLASS\", \"WHO\", \"EMBARK_TOWN\"]\n",
    "num_cols = [\"SIBSP\", \"PARCH\", \"FARE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"CLASS\"  |\"WHO\"  |\"EMBARK_TOWN\"  |\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\n",
      "------------------------------------------------------------------------------\n",
      "|THIRD    |MAN    |SOUTHAMPTON    |0           |1        |0        |7.25     |\n",
      "|FIRST    |WOMAN  |CHERBOURG      |1           |1        |0        |71.2833  |\n",
      "|THIRD    |WOMAN  |SOUTHAMPTON    |1           |0        |0        |7.925    |\n",
      "|FIRST    |WOMAN  |SOUTHAMPTON    |1           |1        |0        |53.1     |\n",
      "|THIRD    |MAN    |SOUTHAMPTON    |0           |0        |0        |8.05     |\n",
      "|THIRD    |MAN    |QUEENSTOWN     |0           |0        |0        |8.4583   |\n",
      "|FIRST    |MAN    |SOUTHAMPTON    |0           |0        |0        |51.8625  |\n",
      "|THIRD    |CHILD  |SOUTHAMPTON    |0           |3        |1        |21.075   |\n",
      "|THIRD    |WOMAN  |SOUTHAMPTON    |1           |0        |2        |11.1333  |\n",
      "|SECOND   |CHILD  |CHERBOURG      |1           |1        |0        |30.0708  |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute_cat = SimpleImputer(\n",
    "    input_cols=cat_cols,\n",
    "    output_cols=cat_cols,\n",
    "    strategy=\"most_frequent\",\n",
    "    drop_input_cols=True,\n",
    ")\n",
    "\n",
    "titanic_df = impute_cat.fit(titanic_df).transform(titanic_df)\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CLASS_SECOND\"  |\"CLASS_THIRD\"  |\"WHO_MAN\"  |\"WHO_WOMAN\"  |\"EMBARK_TOWN_QUEENSTOWN\"  |\"EMBARK_TOWN_SOUTHAMPTON\"  |\"SURVIVED\"  |\"SIBSP\"  |\"PARCH\"  |\"FARE\"   |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|0.0             |1.0            |1.0        |0.0          |0.0                       |1.0                        |0           |1        |0        |7.25     |\n",
      "|0.0             |0.0            |0.0        |1.0          |0.0                       |0.0                        |1           |1        |0        |71.2833  |\n",
      "|0.0             |1.0            |0.0        |1.0          |0.0                       |1.0                        |1           |0        |0        |7.925    |\n",
      "|0.0             |0.0            |0.0        |1.0          |0.0                       |1.0                        |1           |1        |0        |53.1     |\n",
      "|0.0             |1.0            |1.0        |0.0          |0.0                       |1.0                        |0           |0        |0        |8.05     |\n",
      "|0.0             |1.0            |1.0        |0.0          |1.0                       |0.0                        |0           |0        |0        |8.4583   |\n",
      "|0.0             |0.0            |1.0        |0.0          |0.0                       |1.0                        |0           |0        |0        |51.8625  |\n",
      "|0.0             |1.0            |0.0        |0.0          |0.0                       |1.0                        |0           |3        |1        |21.075   |\n",
      "|0.0             |1.0            |0.0        |1.0          |0.0                       |1.0                        |1           |0        |2        |11.1333  |\n",
      "|1.0             |0.0            |0.0        |0.0          |0.0                       |0.0                        |1           |1        |0        |30.0708  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OHE = OneHotEncoder(\n",
    "    input_cols=cat_cols,\n",
    "    output_cols=cat_cols,\n",
    "    drop_input_cols=True,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "titanic_df = OHE.fit(titanic_df).transform(titanic_df)\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = titanic_df.random_split(weights=[0.8, 0.2], seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"max_depth\": list(range(3, 6, 1)),\n",
    "    \"min_child_weight\": list(range(1, 6, 1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100, 200, 300, 400, 500],\n",
       " 'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
       " 'max_depth': [3, 4, 5],\n",
       " 'min_child_weight': [1, 2, 3, 4, 5]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\n",
    "    f\"ALTER WAREHOUSE {session.get_current_warehouse()[1:-1]} SET WAREHOUSE_SIZE=LARGE;\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists may not have the ability to change the warehouse size.  They will usually have access to a larger warehouse and can easily switch as well using session.use_warehouse('bigger_warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'snowflake-snowpark-python' in the local environment is 1.13.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python<2'. Your UDF might not work when the package version is different between the server and your local environment.\n",
      "Package 'fastparquet' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
      "The version of package 'pyarrow' in the local environment is 15.0.1, which does not fit the criteria for the requirement 'pyarrow<14'. Your UDF might not work when the package version is different between the server and your local environment.\n",
      "The version of package 'cachetools' in the local environment is 5.3.3, which does not fit the criteria for the requirement 'cachetools<6'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.ml.modeling.model_selection.grid_search_cv.GridSearchCV at 0x7fb971c460e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(),\n",
    "    param_grid=parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    input_cols=train_df.drop(\"SURVIVED\").columns,\n",
    "    label_cols=\"SURVIVED\",\n",
    "    output_cols=\"PRED_SURVIVED\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\n",
    "    f\"ALTER WAREHOUSE {session.get_current_warehouse()[1:-1]} SET WAREHOUSE_SIZE=XSMALL;\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grid_search.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.819149\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(\n",
    "    df=result, y_true_col_names=\"SURVIVED\", y_pred_col_names=\"PRED_SURVIVED\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.822148</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.820760</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.820760</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.820750</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.820740</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy  learning_rate  max_depth  min_child_weight  n_estimators\n",
       "94   0.822148            0.2          3                 4           500\n",
       "311  0.820760            0.5          3                 3           200\n",
       "216  0.820760            0.3          5                 4           200\n",
       "114  0.820750            0.2          4                 3           500\n",
       "299  0.820740            0.4          5                 5           500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print each combination of hyperparameters with their accuracy\n",
    "results = grid_search.to_sklearn().cv_results_\n",
    "data = {\"accuracy\": results[\"mean_test_score\"]}\n",
    "for i, param in enumerate(results[\"params\"]):\n",
    "    for key, value in param.items():\n",
    "        if key not in data:\n",
    "            data[key] = [None] * len(results[\"params\"])\n",
    "        data[key][i] = value\n",
    "\n",
    "# Create DataFrame\n",
    "hp_df = pd.DataFrame(data).sort_values(by=\"accuracy\", ascending=False)\n",
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = grid_search.to_sklearn().best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to add one to our model versions if it already exists\n",
    "\n",
    "\n",
    "def check_and_update(df, model_name):\n",
    "    if df.empty:\n",
    "        return \"V_1\"\n",
    "    elif df[df[\"name\"] == model_name].empty:\n",
    "        return \"V_1\"\n",
    "    else:\n",
    "        # Increment model_version if df is not a pandas Series\n",
    "        lst = sorted(ast.literal_eval(df[\"versions\"][0]))\n",
    "        last_value = lst[-1]\n",
    "        prefix, num = last_value.rsplit(\"_\", 1)\n",
    "        new_last_value = f\"{prefix}_{int(num)+1}\"\n",
    "        lst[-1] = new_last_value\n",
    "        return new_last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample input data to pass into the registry logging function\n",
    "X = train_df.drop(\"SURVIVED\").limit(100)\n",
    "\n",
    "# Create a registry and log the model\n",
    "# You can specify a different DB and Schema if you'd like\n",
    "# otherwise it uses the session context\n",
    "reg = Registry(session=session)\n",
    "\n",
    "reg_df = reg.show_models()\n",
    "\n",
    "# Define model name and version (use uppercase for name)\n",
    "model_name = \"TITANIC\"\n",
    "\n",
    "model_version = check_and_update(reg_df, model_name)\n",
    "\n",
    "titanic_model = reg.log_model(\n",
    "    model_name=model_name,\n",
    "    version_name=model_version,\n",
    "    model=optimal_model,\n",
    "    sample_input_data=X,\n",
    ")\n",
    "\n",
    "# Add evaluation metric\n",
    "titanic_model.set_metric(\n",
    "    metric_name=\"accuracy\",\n",
    "    value=hp_df[\"accuracy\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>default_version_name</th>\n",
       "      <th>versions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01 07:50:43.635000-08:00</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>SNOWPARK</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>None</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>V_3</td>\n",
       "      <td>[\"V_1\",\"V_2\",\"V_3\",\"V_4\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on     name database_name schema_name comment  \\\n",
       "0 2024-03-01 07:50:43.635000-08:00  TITANIC      SNOWPARK     TITANIC    None   \n",
       "\n",
       "      owner default_version_name                   versions  \n",
       "0  SYSADMIN                  V_3  [\"V_1\",\"V_2\",\"V_3\",\"V_4\"]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    k: v for k, v in optimal_model.get_params().items() if v and k != \"missing\"\n",
    "}\n",
    "titanic_model.set_metric(metric_name=\"hyperparameters\", value=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>module_name</th>\n",
       "      <th>is_default_version</th>\n",
       "      <th>functions</th>\n",
       "      <th>metadata</th>\n",
       "      <th>user_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01 07:50:43.665000-08:00</td>\n",
       "      <td>V_1</td>\n",
       "      <td>None</td>\n",
       "      <td>SNOWPARK</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]</td>\n",
       "      <td>{\"metrics\": {\"accuracy\": 0.8093617021276595, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.2, \"max_depth\": 3, \"min_child_weight\": 4, \"n_estimators\": 500, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}</td>\n",
       "      <td>{\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-01 08:11:43.919000-08:00</td>\n",
       "      <td>V_2</td>\n",
       "      <td>None</td>\n",
       "      <td>SNOWPARK</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT_PROBA\",\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"accuracy\": 0.887931, \"Precision\": 0.8961038961038961, \"Recall\": 0.7931034482758621, \"F1 Score\": 0.8414634146341463, \"Confusion Matrix\": [[411.0, 24.0], [54.0, 207.0]]}, \"snowpark_ml_schema_version\": \"2024-01-01\"}</td>\n",
       "      <td>{\"snowpark_ml_data\":{\"functions\":[{\"name\":\"PREDICT\",\"signature\":{\"inputs\":[{\"name\":\"PCLASS\",\"type\":\"INT8\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"},{\"name\":\"ALONE\",\"type\":\"BOOL\"},{\"name\":\"SEX\",\"type\":\"STRING\"},{\"name\":\"CLASS\",\"type\":\"STRING\"},{\"name\":\"WHO\",\"type\":\"STRING\"},{\"name\":\"EMBARK_TOWN\",\"type\":\"STRING\"}],\"outputs\":[{\"name\":\"SEX_MALE\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-04 13:46:48.044000-08:00</td>\n",
       "      <td>V_3</td>\n",
       "      <td>None</td>\n",
       "      <td>SNOWPARK</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>true</td>\n",
       "      <td>[\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]</td>\n",
       "      <td>{\"metrics\": {\"accuracy\": 0.8014285714285714, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.5, \"max_depth\": 4, \"min_child_weight\": 3, \"n_estimators\": 100, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}</td>\n",
       "      <td>{\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-14 06:37:27.315000-07:00</td>\n",
       "      <td>V_4</td>\n",
       "      <td>None</td>\n",
       "      <td>SNOWPARK</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>TITANIC</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]</td>\n",
       "      <td>{\"metrics\": {\"accuracy\": 0.8093617021276595, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.2, \"max_depth\": 3, \"min_child_weight\": 4, \"n_estimators\": 500, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}</td>\n",
       "      <td>{\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on name comment database_name schema_name  \\\n",
       "0 2024-03-01 07:50:43.665000-08:00  V_1    None      SNOWPARK     TITANIC   \n",
       "1 2024-03-01 08:11:43.919000-08:00  V_2    None      SNOWPARK     TITANIC   \n",
       "2 2024-03-04 13:46:48.044000-08:00  V_3    None      SNOWPARK     TITANIC   \n",
       "3 2024-03-14 06:37:27.315000-07:00  V_4    None      SNOWPARK     TITANIC   \n",
       "\n",
       "  module_name is_default_version                            functions  \\\n",
       "0     TITANIC              false  [\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]   \n",
       "1     TITANIC              false          [\"PREDICT_PROBA\",\"PREDICT\"]   \n",
       "2     TITANIC               true  [\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]   \n",
       "3     TITANIC              false  [\"PREDICT_PROBA\",\"PREDICT\",\"APPLY\"]   \n",
       "\n",
       "                                                                                                                                                                                                                                        metadata  \\\n",
       "0  {\"metrics\": {\"accuracy\": 0.8093617021276595, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.2, \"max_depth\": 3, \"min_child_weight\": 4, \"n_estimators\": 500, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}   \n",
       "1             {\"metrics\": {\"accuracy\": 0.887931, \"Precision\": 0.8961038961038961, \"Recall\": 0.7931034482758621, \"F1 Score\": 0.8414634146341463, \"Confusion Matrix\": [[411.0, 24.0], [54.0, 207.0]]}, \"snowpark_ml_schema_version\": \"2024-01-01\"}   \n",
       "2  {\"metrics\": {\"accuracy\": 0.8014285714285714, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.5, \"max_depth\": 4, \"min_child_weight\": 3, \"n_estimators\": 100, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}   \n",
       "3  {\"metrics\": {\"accuracy\": 0.8093617021276595, \"hyperparameters\": {\"objective\": \"binary:logistic\", \"learning_rate\": 0.2, \"max_depth\": 3, \"min_child_weight\": 4, \"n_estimators\": 500, \"n_jobs\": 3}}, \"snowpark_ml_schema_version\": \"2024-01-01\"}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             user_data  \n",
       "0  {\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...  \n",
       "1  {\"snowpark_ml_data\":{\"functions\":[{\"name\":\"PREDICT\",\"signature\":{\"inputs\":[{\"name\":\"PCLASS\",\"type\":\"INT8\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"},{\"name\":\"ALONE\",\"type\":\"BOOL\"},{\"name\":\"SEX\",\"type\":\"STRING\"},{\"name\":\"CLASS\",\"type\":\"STRING\"},{\"name\":\"WHO\",\"type\":\"STRING\"},{\"name\":\"EMBARK_TOWN\",\"type\":\"STRING\"}],\"outputs\":[{\"name\":\"SEX_MALE\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"n...  \n",
       "2  {\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...  \n",
       "3  {\"snowpark_ml_data\":{\"functions\":[{\"name\":\"APPLY\",\"signature\":{\"inputs\":[{\"name\":\"CLASS_SECOND\",\"type\":\"DOUBLE\"},{\"name\":\"CLASS_THIRD\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_MAN\",\"type\":\"DOUBLE\"},{\"name\":\"WHO_WOMAN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_QUEENSTOWN\",\"type\":\"DOUBLE\"},{\"name\":\"EMBARK_TOWN_SOUTHAMPTON\",\"type\":\"DOUBLE\"},{\"name\":\"SIBSP\",\"type\":\"INT8\"},{\"name\":\"PARCH\",\"type\":\"INT8\"},{\"name\":\"FARE\",\"type\":\"DOUBLE\"}],\"outputs\":[{\"name\":\"output_feature_0\",\"type\":\"FLOAT\"},{\"name\":\"output_featur...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "reg.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple versions of the model, we want the UDF to be deployed as the version with the highest accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = reg.get_model(model_name).show_versions()\n",
    "reg_df[\"accuracy\"] = reg_df[\"metadata\"].apply(\n",
    "    lambda x: json.loads(x)[\"metrics\"][\"accuracy\"]\n",
    ")\n",
    "best_model = reg_df.sort_values(by=\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployed_version = best_model[\"name\"].iloc[0]\n",
    "deployed_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default version to the deployed version (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = reg.get_model(model_name)\n",
    "m.default = deployed_version\n",
    "mv = m.default\n",
    "mv.version_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "(2112) \nData Validation Error when validating your Snowpark DataFrame.\nIf using the normalized names from model signatures, there are the following errors:\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\n\nIf using the inferred names from model signatures, there are the following errors:\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowflakeMLException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/Intro_SnowML/lib/python3.10/site-packages/snowflake/ml/_internal/telemetry.py:358\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Intro_SnowML/lib/python3.10/site-packages/snowflake/ml/model/_client/model/model_version_impl.py:331\u001b[0m, in \u001b[0;36mModelVersion.run\u001b[0;34m(self, X, function_name, strict_input_validation)\u001b[0m\n\u001b[1;32m    330\u001b[0m     target_function_info \u001b[38;5;241m=\u001b[39m functions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_identifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSqlIdentifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_function_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_function_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_version_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict_input_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict_input_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Intro_SnowML/lib/python3.10/site-packages/snowflake/ml/model/_client/ops/model_ops.py:401\u001b[0m, in \u001b[0;36mModelOperator.invoke_method\u001b[0;34m(self, method_name, signature, X, model_name, version_name, strict_input_validation, statement_params)\u001b[0m\n\u001b[1;32m    400\u001b[0m output_with_input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m identifier_rule \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_signature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_snowpark_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict_input_validation\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m s_df \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Intro_SnowML/lib/python3.10/site-packages/snowflake/ml/model/model_signature.py:469\u001b[0m, in \u001b[0;36m_validate_snowpark_data\u001b[0;34m(data, features, strict)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(error_list) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m error_list \u001b[38;5;129;01min\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 469\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[1;32m    470\u001b[0m             error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mINVALID_DATA,\n\u001b[1;32m    471\u001b[0m             original_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124mData Validation Error when validating your Snowpark DataFrame.\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124mIf using the normalized names from model signatures, there are the following errors:\u001b[39m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;132;01m{\u001b[39;00merrors[SnowparkIdentifierRule\u001b[38;5;241m.\u001b[39mNORMALIZED]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \n\u001b[1;32m    477\u001b[0m \u001b[38;5;124mIf using the inferred names from model signatures, there are the following errors:\u001b[39m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;132;01m{\u001b[39;00merrors[SnowparkIdentifierRule\u001b[38;5;241m.\u001b[39mINFERRED]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    480\u001b[0m             ),\n\u001b[1;32m    481\u001b[0m         )\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSnowflakeMLException\u001b[0m: ValueError(\"(2112) \\nData Validation Error when validating your Snowpark DataFrame.\\nIf using the normalized names from model signatures, there are the following errors:\\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\\n\\nIf using the inferred names from model signatures, there are the following errors:\\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\\n\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m remote_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict_proba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m remote_prediction\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Intro_SnowML/lib/python3.10/site-packages/snowflake/ml/_internal/telemetry.py:380\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_stmt_params_if_snowpark_df(res, statement_params)\n",
      "\u001b[0;31mValueError\u001b[0m: (2112) \nData Validation Error when validating your Snowpark DataFrame.\nIf using the normalized names from model signatures, there are the following errors:\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\n\nIf using the inferred names from model signatures, there are the following errors:\n[ValueError('Data Validation Error: feature PCLASS does not exist in data.'), ValueError('Data Validation Error: feature ALONE does not exist in data.'), ValueError('Data Validation Error: feature SEX does not exist in data.'), ValueError('Data Validation Error: feature CLASS does not exist in data.'), ValueError('Data Validation Error: feature WHO does not exist in data.'), ValueError('Data Validation Error: feature EMBARK_TOWN does not exist in data.')]\n"
     ]
    }
   ],
   "source": [
    "remote_prediction = mv.run(test_df, function_name=\"predict_proba\")\n",
    "remote_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test in SQL write test data back to a table\n",
    "\n",
    "test_df.write.mode(\"overwrite\").save_as_table(\"TEST_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add images to stage for Streamlit App\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.file.put(\"../streamlit_images/*\", \"@ML_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling model from a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the registry\n",
    "\n",
    "reg = Registry(session=session)\n",
    "\n",
    "# Get the default version of your model (Model with best accuracy in our case)\n",
    "\n",
    "mv = reg.get_model(\"titanic\").default\n",
    "\n",
    "remote_prediction = mv.run(test_df, function_name=\"predict_proba\")\n",
    "remote_prediction.drop('\"output_feature_0\"').with_column_renamed(\n",
    "    '\"output_feature_1\"', \"pred_survived\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To delete your model and all of it's versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg.delete_model(\"TITANIC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intro_SnowML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
